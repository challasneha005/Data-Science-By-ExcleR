{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGcnDxuzbTKn",
        "outputId": "39f50cb4-c26b-467d-9e80-c1f1cfe6904c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities (text, label):\n",
            "Apple Inc.                -> ORG\n",
            "Hyderabad                 -> GPE\n",
            "India                     -> GPE\n",
            "Tim Cook                  -> PERSON\n",
            "March 2023                -> DATE\n",
            "5,000                     -> CARDINAL\n",
            "\n",
            "Entities grouped by label:\n",
            "ORG: ['Apple Inc.']\n",
            "GPE: ['Hyderabad', 'India']\n",
            "PERSON: ['Tim Cook']\n",
            "DATE: ['March 2023']\n",
            "CARDINAL: ['5,000']\n"
          ]
        }
      ],
      "source": [
        "# NER with spaCy\n",
        "# Setup (run in terminal once): python -m pip install spacy\n",
        "# Download a small English model: python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"\"\"\n",
        "Apple Inc. plans a new office in Hyderabad, India. Tim Cook announced this in March 2023.\n",
        "The site will create 5,000 jobs and focus on innovative technologies in supply chain analytics.\n",
        "\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Named Entities (text, label):\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text:25} -> {ent.label_}\")\n",
        "\n",
        "# Optional: group entities by label for summarization\n",
        "from collections import defaultdict\n",
        "by_label = defaultdict(list)\n",
        "for ent in doc.ents:\n",
        "    by_label[ent.label_].append(ent.text)\n",
        "\n",
        "print(\"\\nEntities grouped by label:\")\n",
        "for label, items in by_label.items():\n",
        "    print(f\"{label}: {sorted(set(items))}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word embeddings with gensim Word2Vec\n",
        "# Setup: python -m pip install gensim nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\") # Added to resolve LookupError\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "corpus = \"\"\"\n",
        "Manufacturing relies on predictive maintenance and supply chain optimization.\n",
        "Data engineers build pipelines, while analysts monitor KPIs and anomalies.\n",
        "Robotics and IoT sensors stream telemetry to cloud databases for real-time insights.\n",
        "Quality control uses computer vision to detect defects on the shop floor.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize sentences -> words\n",
        "sentences = [word_tokenize(s.lower()) for s in sent_tokenize(corpus)]\n",
        "\n",
        "# Train a small Word2Vec model\n",
        "model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=50,   # embedding dimension\n",
        "    window=5,         # context window size\n",
        "    min_count=1,      # keep all words for demo\n",
        "    workers=2,\n",
        "    sg=1              # skip-gram; use 0 for CBOW\n",
        ")\n",
        "\n",
        "# Explore similar words\n",
        "for target in [\"maintenance\", \"supply\", \"quality\", \"telemetry\"]:\n",
        "    print(f\"\\nTop similar to '{target}':\")\n",
        "    try:\n",
        "        for w, score in model.wv.most_similar(target, topn=5):\n",
        "            print(f\"{w:15} -> {score:.3f}\")\n",
        "    except KeyError:\n",
        "        print(\"Word not in vocabulary.\")\n",
        "\n",
        "# Cosine similarity between pairs\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine(u, v):\n",
        "    return dot(u, v) / (norm(u) * norm(v))\n",
        "\n",
        "pairs = [(\"maintenance\", \"telemetry\"),\n",
        "         (\"quality\", \"defects\"),\n",
        "         (\"supply\", \"optimization\")]\n",
        "\n",
        "print(\"\\nCosine similarities:\")\n",
        "for a, b in pairs:\n",
        "    try:\n",
        "        sim = cosine(model.wv[a], model.wv[b])\n",
        "        print(f\"{a:12} ~ {b:12} -> {sim:.3f}\")\n",
        "    except KeyError:\n",
        "        print(f\"Missing word: {a} or {b}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LURJ5mDQbXrz",
        "outputId": "17081230-d644-43ba-f707-c2e21ed97744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top similar to 'maintenance':\n",
            "real-time       -> 0.240\n",
            "defects         -> 0.240\n",
            "analysts        -> 0.236\n",
            "kpis            -> 0.224\n",
            "detect          -> 0.204\n",
            "\n",
            "Top similar to 'supply':\n",
            "optimization    -> 0.286\n",
            "to              -> 0.252\n",
            "cloud           -> 0.192\n",
            "analysts        -> 0.169\n",
            "maintenance     -> 0.169\n",
            "\n",
            "Top similar to 'quality':\n",
            "shop            -> 0.307\n",
            "cloud           -> 0.267\n",
            "uses            -> 0.191\n",
            "floor           -> 0.154\n",
            ".               -> 0.151\n",
            "\n",
            "Top similar to 'telemetry':\n",
            "predictive      -> 0.171\n",
            "cloud           -> 0.171\n",
            "manufacturing   -> 0.167\n",
            "iot             -> 0.166\n",
            "for             -> 0.164\n",
            "\n",
            "Cosine similarities:\n",
            "maintenance  ~ telemetry    -> -0.145\n",
            "quality      ~ defects      -> 0.119\n",
            "supply       ~ optimization -> 0.286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b1cd676",
        "outputId": "37c0a239-1dc2-4926-8b0c-3a97270a10ae"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Demo (Trigram counts)\n",
        "from collections import defaultdict\n",
        "\n",
        "corpus = \"the cat sat on the mat the cat lay on the rug the dog barked loudly\"\n",
        "tokens = corpus.split()\n",
        "\n",
        "# Build trigram counts\n",
        "trigram_counts = defaultdict(lambda: defaultdict(int))\n",
        "for i in range(len(tokens)-2):\n",
        "    context = (tokens[i], tokens[i+1])\n",
        "    next_word = tokens[i+2]\n",
        "    trigram_counts[context][next_word] += 1\n",
        "\n",
        "# Predict next word after \"the cat\"\n",
        "context = (\"the\", \"cat\")\n",
        "print(\"Next word predictions for context:\", context)\n",
        "for word, count in trigram_counts[context].items():\n",
        "    print(f\"{word} -> {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52Kw7t-1fDca",
        "outputId": "69c1e935-ba97-4656-e211-d15e921ce42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next word predictions for context: ('the', 'cat')\n",
            "sat -> 1\n",
            "lay -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Demo (Simple feedforward LM with Keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
        "\n",
        "# Vocabulary size and embedding dimension\n",
        "vocab_size = 50\n",
        "embed_dim = 8\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=2), # context length=2\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(vocab_size, activation='softmax')  # predict next word\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "model.build(input_shape=(None, 2))  # batch size flexible, sequence length = 2\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "GYmEYjUqmDbv",
        "outputId": "a430c240-34d1-419b-ffce-fdc7da67229d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m)           │           \u001b[38;5;34m400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m544\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m1,650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,594\u001b[0m (10.13 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,594</span> (10.13 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,594\u001b[0m (10.13 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,594</span> (10.13 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code Demo (LSTM for next‑word prediction)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Vocabulary size and embedding dimension\n",
        "vocab_size = 100\n",
        "embed_dim = 16\n",
        "seq_length = 5  # input sequence length\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=seq_length),\n",
        "    LSTM(64),  # LSTM layer with 64 units\n",
        "    Dense(vocab_size, activation='softmax')  # predict next word\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Force the model to build by providing input shape\n",
        "model.build(input_shape=(None, seq_length))\n",
        "\n",
        "# Show the summary\n",
        "model.summary()\n",
        "\n",
        "# Optional: run a dummy prediction to confirm\n",
        "dummy_input = np.random.randint(0, vocab_size, (1, seq_length))\n",
        "print(\"Dummy prediction shape:\", model.predict(dummy_input).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Tp_S67aMmM12",
        "outputId": "092a216c-6ff4-4172-9f60-235d9712ec13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │         \u001b[38;5;34m1,600\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m20,736\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m6,500\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,736</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,500</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28,836\u001b[0m (112.64 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,836</span> (112.64 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28,836\u001b[0m (112.64 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,836</span> (112.64 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step\n",
            "Dummy prediction shape: (1, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HeQmTlTx6vL1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}